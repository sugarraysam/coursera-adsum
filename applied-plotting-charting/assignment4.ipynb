{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4\n",
    "\n",
    "Before working on this assignment please read these instructions fully. In the submission area, you will notice that you can click the link to **Preview the Grading** for each step of the assignment. This is the criteria that will be used for peer grading. Please familiarize yourself with the criteria before beginning the assignment.\n",
    "\n",
    "This assignment requires that you to find **at least** two datasets on the web which are related, and that you visualize these datasets to answer a question with the broad topic of **sports or athletics** (see below) for the region of **Mont-Tremblant, Quebec, Canada**, or **Canada** more broadly.\n",
    "\n",
    "You can merge these datasets with data from different regions if you like! For instance, you might want to compare **Mont-Tremblant, Quebec, Canada** to Ann Arbor, USA. In that case at least one source file must be about **Mont-Tremblant, Quebec, Canada**.\n",
    "\n",
    "You are welcome to choose datasets at your discretion, but keep in mind **they will be shared with your peers**, so choose appropriate datasets. Sensitive, confidential, illicit, and proprietary materials are not good choices for datasets for this assignment. You are welcome to upload datasets of your own as well, and link to them using a third party repository such as github, bitbucket, pastebin, etc. Please be aware of the Coursera terms of service with respect to intellectual property.\n",
    "\n",
    "Also, you are welcome to preserve data in its original language, but for the purposes of grading you should provide english translations. You are welcome to provide multiple visuals in different languages if you would like!\n",
    "\n",
    "As this assignment is for the whole course, you must incorporate principles discussed in the first week, such as having as high data-ink ratio (Tufte) and aligning with Cairoâ€™s principles of truth, beauty, function, and insight.\n",
    "\n",
    "Here are the assignment instructions:\n",
    "\n",
    " * State the region and the domain category that your data sets are about (e.g., **Mont-Tremblant, Quebec, Canada** and **sports or athletics**).\n",
    " * You must state a question about the domain category and region that you identified as being interesting.\n",
    " * You must provide at least two links to available datasets. These could be links to files such as CSV or Excel files, or links to websites which might have data in tabular form, such as Wikipedia pages.\n",
    " * You must upload an image which addresses the research question you stated. In addition to addressing the question, this visual should follow Cairo's principles of truthfulness, functionality, beauty, and insightfulness.\n",
    " * You must contribute a short (1-2 paragraph) written justification of how your visualization addresses your stated research question.\n",
    "\n",
    "What do we mean by **sports or athletics**?  For this category we are interested in sporting events or athletics broadly, please feel free to creatively interpret the category when building your research question!\n",
    "\n",
    "## Tips\n",
    "* Wikipedia is an excellent source of data, and I strongly encourage you to explore it for new data sources.\n",
    "* Many governments run open data initiatives at the city, region, and country levels, and these are wonderful resources for localized data sources.\n",
    "* Several international agencies, such as the [United Nations](http://data.un.org/), the [World Bank](http://data.worldbank.org/), the [Global Open Data Index](http://index.okfn.org/place/) are other great places to look for data.\n",
    "* This assignment requires you to convert and clean datafiles. Check out the discussion forums for tips on how to do this from various sources, and share your successes with your fellow students!\n",
    "\n",
    "## Example\n",
    "Looking for an example? Here's what our course assistant put together for the **Ann Arbor, MI, USA** area using **sports and athletics** as the topic. [Example Solution File](./readonly/Assignment4_example.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Dataset 1**\n",
    "- Ironman 2018 results in Mont-Tremblant per categories\n",
    "- https://www.endurance-data.com/en/results/270-ironman-mont-tremblant/all/\n",
    "    \n",
    "**Dataset 2**\n",
    "- Distance from Canada to other countries\n",
    "- https://www.distancefromto.net/distance-from-canada-country\n",
    "    \n",
    "**Dataset 3**\n",
    "- List of FIPS country codes\n",
    "- https://en.wikipedia.org/wiki/List_of_FIPS_country_codes\n",
    "\n",
    "**Dataset 4**\n",
    "- List of ISO 3166-1 alpha-3 country codes\n",
    "- https://en.wikipedia.org/wiki/ISO_3166-1_alpha-3\n",
    "    \n",
    "**TODO**\n",
    "- clean ++ general describe on datasets\n",
    "- piecharts avg nationalities (total)\n",
    "- scatter plot everything vs categories (should see clusters especially for time results)\n",
    "- % of country represented\n",
    "- join df_distance && df_ironman\n",
    "- general correlation plot every column vs. distance from Canada (international athletes perform better, per category)\n",
    "- Do international atheletes that travel further from their home country perform better than local atheletes? \n",
    "- markdown cell explaining datasets & hypothesis formulation & p-value\n",
    "    -- did not use more recent weather data because required lots of cleaning and parsing (NOAA, NCEI)\n",
    "- groupby category, get mean + std + preview distributions\n",
    "- compute p-value and reject/fail-to-reject hypothesis\n",
    "- markdown cell explaining results and visualizations & limitations\n",
    "    = Limitations\n",
    "        -> calculating distance from country of origin, not town where athele lives, \n",
    "            even if still lives in home country, not using hometown (very rough estimate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Load & clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "// Disable scrolling\n",
       "//    https://stackoverflow.com/questions/41641205/how-to-avoid-output-into-scrollable-frames-in-jupyter-notebook\n",
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"%%javascript\\n// Disable scrolling\\n//    https://stackoverflow.com/questions/41641205/how-to-avoid-output-into-scrollable-frames-in-jupyter-notebook\\nIPython.OutputArea.prototype._should_scroll = function(lines) {\\n    return false;\\n}\";\n",
       "                var nbb_formatted_code = \"%%javascript\\n// Disable scrolling\\n//    https://stackoverflow.com/questions/41641205/how-to-avoid-output-into-scrollable-frames-in-jupyter-notebook\\nIPython.OutputArea.prototype._should_scroll = function(lines) {\\n    return false;\\n}\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "// Disable scrolling\n",
    "//    https://stackoverflow.com/questions/41641205/how-to-avoid-output-into-scrollable-frames-in-jupyter-notebook\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nb_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext nb_black\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 103;\n",
       "                var nbb_unformatted_code = \"import pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n%matplotlib notebook\\n%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"import pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n%matplotlib notebook\\n%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib notebook\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__call__() takes from 1 to 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-162-9c50204a4698>\u001b[0m in \u001b[0;36m_alpha_to_fips\u001b[0;34m(self, code)\u001b[0m\n\u001b[1;32m     16\u001b[0m             return self.countries['FIPS'][self.countries['Alpha'] == code\n\u001b[0;32m---> 17\u001b[0;31m                                          ].values[0]\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-162-9c50204a4698>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mironman\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-162-9c50204a4698>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcountries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_countries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mironman\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_ironman\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_dists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-162-9c50204a4698>\u001b[0m in \u001b[0;36m_load_ironman\u001b[0;34m(self, fname)\u001b[0m\n\u001b[1;32m     41\u001b[0m                 {'Country': lambda x: self._alpha_to_fips(x.strip())}, **{\n\u001b[1;32m     42\u001b[0m                     \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"%H:%M:%S\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Swim'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Bike'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Run'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                 }\n\u001b[1;32m     45\u001b[0m             )\n",
      "\u001b[0;32m~/opt/coursera-adsum/.venv/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    643\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/coursera-adsum/.venv/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m     \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/coursera-adsum/.venv/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    936\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skipfooter not supported for iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 938\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'as_recarray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/coursera-adsum/.venv/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1503\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1505\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1506\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.read (pandas/parser.c:9884)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._read_low_memory (pandas/parser.c:10142)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._read_rows (pandas/parser.c:11161)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._convert_column_data (pandas/parser.c:12503)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser._apply_converter (pandas/parser.c:27495)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-162-9c50204a4698>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     39\u001b[0m             },\n\u001b[1;32m     40\u001b[0m             converters=dict(\n\u001b[0;32m---> 41\u001b[0;31m                 {'Country': lambda x: self._alpha_to_fips(x.strip())}, **{\n\u001b[0m\u001b[1;32m     42\u001b[0m                     \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"%H:%M:%S\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Swim'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Bike'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Run'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-162-9c50204a4698>\u001b[0m in \u001b[0;36m_alpha_to_fips\u001b[0;34m(self, code)\u001b[0m\n\u001b[1;32m     17\u001b[0m                                          ].values[0]\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_load_alpha3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'data/alpha3.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __call__() takes from 1 to 2 positional arguments but 3 were given"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 162;\n",
       "                var nbb_unformatted_code = \"class Dataloader():\\n    def __init__(self):\\n        self.countries = self._load_countries()\\n        self.ironman = self._load_ironman()\\n        self.distances = self._load_dists()\\n\\n    def _load_countries(self):\\n        df = pd.merge(self._load_alpha3(), self._load_fips(), on='Name')\\n        return df\\n\\n    def _fips_to_alpha(self, code):\\n        return self.countries['Alpha'][self.countries['FIPS'] == code].values[0]\\n\\n    def _alpha_to_fips(self, code):\\n        try:\\n            return self.countries['FIPS'][self.countries['Alpha'] == code\\n                                         ].values[0]\\n        except Exception as e:\\n            exit(code, code.replace(' ', '_'))\\n\\n    def _load_alpha3(self, fname='data/alpha3.csv'):\\n        df = pd.read_csv(fname, names=['Alpha', 'Name'])\\n        return df\\n\\n    def _load_fips(self, fname='data/FIPS.csv'):\\n        df = pd.read_csv(fname, names=['FIPS', 'Name'])\\n        return df\\n\\n    def _load_ironman(self, fname='data/ironman2018.csv'):\\n        # TODO convert alpha3 -> FIPS\\n        df = pd.read_csv(\\n            fname,\\n            names=[\\n                'Overall', 'GenderRank', 'DivRank', 'Name', 'BIB', 'AgeGroup',\\n                'Country', 'Swim', 'Bike', 'Run', 'Total'\\n            ],\\n            dtype={\\n                'Overall': np.int64,\\n            },\\n            converters=dict(\\n                {'Country': lambda x: self._alpha_to_fips(x.strip())}, **{\\n                    k: lambda x: pd.to_datetime(x, format=\\\"%H:%M:%S\\\").time()\\n                    for k in ['Swim', 'Bike', 'Run']\\n                }\\n            )\\n        )\\n        return df\\n\\n    def _load_dists(self, fname='data/distances.csv'):\\n        df = pd.read_csv(fname)\\n        return df\\n\\n\\ndl = Dataloader()\\ndl.ironman.head()\";\n",
       "                var nbb_formatted_code = \"class Dataloader():\\n    def __init__(self):\\n        self.countries = self._load_countries()\\n        self.ironman = self._load_ironman()\\n        self.distances = self._load_dists()\\n\\n    def _load_countries(self):\\n        df = pd.merge(self._load_alpha3(), self._load_fips(), on='Name')\\n        return df\\n\\n    def _fips_to_alpha(self, code):\\n        return self.countries['Alpha'][self.countries['FIPS'] == code].values[0]\\n\\n    def _alpha_to_fips(self, code):\\n        try:\\n            return self.countries['FIPS'][self.countries['Alpha'] == code\\n                                         ].values[0]\\n        except Exception as e:\\n            exit(code, code.replace(' ', '_'))\\n\\n    def _load_alpha3(self, fname='data/alpha3.csv'):\\n        df = pd.read_csv(fname, names=['Alpha', 'Name'])\\n        return df\\n\\n    def _load_fips(self, fname='data/FIPS.csv'):\\n        df = pd.read_csv(fname, names=['FIPS', 'Name'])\\n        return df\\n\\n    def _load_ironman(self, fname='data/ironman2018.csv'):\\n        # TODO convert alpha3 -> FIPS\\n        df = pd.read_csv(\\n            fname,\\n            names=[\\n                'Overall', 'GenderRank', 'DivRank', 'Name', 'BIB', 'AgeGroup',\\n                'Country', 'Swim', 'Bike', 'Run', 'Total'\\n            ],\\n            dtype={\\n                'Overall': np.int64,\\n            },\\n            converters=dict(\\n                {'Country': lambda x: self._alpha_to_fips(x.strip())}, **{\\n                    k: lambda x: pd.to_datetime(x, format=\\\"%H:%M:%S\\\").time()\\n                    for k in ['Swim', 'Bike', 'Run']\\n                }\\n            )\\n        )\\n        return df\\n\\n    def _load_dists(self, fname='data/distances.csv'):\\n        df = pd.read_csv(fname)\\n        return df\\n\\n\\ndl = Dataloader()\\ndl.ironman.head()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Dataloader():\n",
    "    def __init__(self):\n",
    "        self.countries = self._load_countries()\n",
    "        self.ironman = self._load_ironman()\n",
    "        self.distances = self._load_dists()\n",
    "\n",
    "    def _load_countries(self):\n",
    "        df = pd.merge(self._load_alpha3(), self._load_fips(), on='Name')\n",
    "        return df\n",
    "\n",
    "    def _fips_to_alpha(self, code):\n",
    "        return self.countries['Alpha'][self.countries['FIPS'] == code].values[0]\n",
    "\n",
    "    def _alpha_to_fips(self, code):\n",
    "        try:\n",
    "            return self.countries['FIPS'][self.countries['Alpha'] == code\n",
    "                                         ].values[0]\n",
    "        except Exception as e:\n",
    "            # TODO print error and stop program\n",
    "            pass\n",
    "\n",
    "    def _load_alpha3(self, fname='data/alpha3.csv'):\n",
    "        df = pd.read_csv(fname, names=['Alpha', 'Name'])\n",
    "        return df\n",
    "\n",
    "    def _load_fips(self, fname='data/FIPS.csv'):\n",
    "        df = pd.read_csv(fname, names=['FIPS', 'Name'])\n",
    "        return df\n",
    "\n",
    "    def _load_ironman(self, fname='data/ironman2018.csv'):\n",
    "        # TODO convert alpha3 -> FIPS\n",
    "        # - strip whitespace\n",
    "        df = pd.read_csv(\n",
    "            fname,\n",
    "            names=[\n",
    "                'Overall', 'GenderRank', 'DivRank', 'Name', 'BIB', 'AgeGroup',\n",
    "                'Country', 'Swim', 'Bike', 'Run', 'Total'\n",
    "            ],\n",
    "            dtype={\n",
    "                'Overall': np.int64,\n",
    "            },\n",
    "            converters=dict(\n",
    "                {'Country': lambda x: self._alpha_to_fips(x.strip())}, **{\n",
    "                    k: lambda x: pd.to_datetime(x, format=\"%H:%M:%S\").time()\n",
    "                    for k in ['Swim', 'Bike', 'Run']\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    def _load_dists(self, fname='data/distances.csv'):\n",
    "        df = pd.read_csv(fname)\n",
    "        return df\n",
    "\n",
    "\n",
    "dl = Dataloader()\n",
    "dl.ironman.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
